{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aloumon/CS5542/blob/master/ICP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZrrtyA-Ihi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn36mR6gNhiq",
        "colab_type": "text"
      },
      "source": [
        "In this example, we will be using the famous CIFAR-10 dataset. CIFAR-10 is a large image dataset containing over 60,000 images representing 10 different classes of objects like cats, planes, and cars.\n",
        "\n",
        "The images are full-color RGB, but they are fairly small, only 32 x 32. One great thing about the CIFAR-10 dataset is that it comes prepackaged with Keras, so it is very easy to load up the dataset and the images need very little preprocessing.\n",
        "\n",
        "We're going to be using a random seed here so that the results achieved in this ICP can be replicated by you,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agVj8xIAGu4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set random seed for purposes of reproducibility\n",
        "seed = 21"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smw-A_L8OXRZ",
        "colab_type": "text"
      },
      "source": [
        "Now let's load in the dataset. We can do so simply by specifying which variables we want to load the data into, and then using the load_data() function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1G2V0ldG-gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading in the data\n",
        "     \n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqt5wTAHKyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "acf73399-eada-4b16-f5cf-be9aa0f6fac2"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CXWy03Hdnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c7093611-7a26-40f3-93b8-6fcdc46f4199"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 1)\n",
            "(10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdQOTwqWHmVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "068491a5-aafa-43e6-fd6f-23aee6f0c08d"
      },
      "source": [
        "print (y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3]\n",
            " [8]\n",
            " [8]\n",
            " ...\n",
            " [5]\n",
            " [1]\n",
            " [7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8fwz7HPDM4",
        "colab_type": "text"
      },
      "source": [
        "In most cases you will need to do some preprocessing of your data to get it ready for use, but since we are using a prepackaged dataset, very little preprocessing needs to be done. One thing we want to do is normalize the input data.\n",
        "\n",
        "If the values of the input data are in too wide a range it can negatively impact how the network performs. In this case, the input values are the pixels in the image, which have a value between 0 to 255.\n",
        "\n",
        "So in order to normalize the data we can simply divide the image values by 255. To do this we first need to make the data a float type, since they are currently integers. We can do this by using the astype() Numpy command and then declaring what data type we want:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu-SWHIoHd0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # normalize the inputs from 0-255 to between 0 and 1 by dividing by 255   \n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggDYGPFnP3rb",
        "colab_type": "text"
      },
      "source": [
        "The Numpy command to_categorical() is used to one-hot encode. This is why we imported the np_utils function from Keras, as it contains to_categorical().\n",
        "\n",
        "We also need to specify the number of classes that are in the dataset, so we know how many neurons to compress the final layer down to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AU4mn3WIaL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "class_num = y_test.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRnX3Eo0QlRP",
        "colab_type": "text"
      },
      "source": [
        "Designing the Model\n",
        " Keras has several different formats or blueprints to build models on, but Sequential is the most commonly used, and for that reason, we have imported it from Keras.\n",
        "\n",
        " The first layer of our model is a convolutional layer. It will take in the inputs and run convolutional filters on them.\n",
        "\n",
        "When implementing these in Keras, we have to specify the number of channels/filters we want (that's the 32 below), the size of the filter we want (3 x 3 in this case), the input shape (when creating the first layer) and the activation and padding we need.\n",
        "\n",
        "As mentioned, relu is the most common activation, and padding='same' just means we aren't changing the size of the image at all:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-S1CPtSI6xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# you can also string the activations and poolings together, like this:\n",
        "#model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V9CZIipRubT",
        "colab_type": "text"
      },
      "source": [
        "Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers (0.2 means it drops 20% of the existing connections).\n",
        "\n",
        "We may also want to do batch normalization here. Batch Normalization normalizes the inputs heading into the next layer, ensuring that the network always creates activations with the same distribution that we desire.\n",
        "\n",
        "Then comes another convolutional layer, but the filter size increases so the network can learn more complex representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjesQqmOJLFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M07g4JsYScEP",
        "colab_type": "text"
      },
      "source": [
        "Here's the pooling layer,  this helps make the image classifier more robust so it can learn relevant patterns. There's also the dropout and batch normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdwlhiPeJZl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytSUrc42TD0e",
        "colab_type": "text"
      },
      "source": [
        "You can vary the exact number of convolutional layers you have to your liking, though each one adds more computation expenses. Notice that as you add convolutional layers you typically increase their number of filters so the model can learn more complex representations. If the numbers chosen for these layers seems somewhat arbitrary, just know that in general, you increase filters as you go on and it's advised to make them powers of 2 which can grant a slight benefit when training on a GPU.\n",
        "\n",
        "It's important not to have too many pooling layers, as each pooling discards some data. Pooling too often will lead to there being almost nothing for the densely connected layers to learn about when the data reaches them.\n",
        "\n",
        "The exact number of pooling layers you should use will vary depending on the task you are doing, and it's something you'll get a feel for over time. Since the images are so small here already we won't pool more than twice.\n",
        "\n",
        "You can now repeat these layers to give your network more representations to work off of:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8FpihWJgUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLwZyb_6TOh-",
        "colab_type": "text"
      },
      "source": [
        "After we are done with the convolutional layers, we need to Flatten the data, which is why we imported the function above. We'll also add a layer of dropout again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuRu5l5wJo4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeXEx91fTRMA",
        "colab_type": "text"
      },
      "source": [
        "Now we make use of the Dense import and create the first densely connected layer. We need to specify the number of neurons in the dense layer. Note that the numbers of neurons in succeeding layers decreases, eventually approaching the same number of neurons as there are classes in the dataset (in this case 10). The kernel constraint can regularize the data as it learns, another thing that helps prevent overfitting. This is why we imported maxnorm earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bslgsk4QJuHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ujgsc_T-UD",
        "colab_type": "text"
      },
      "source": [
        "In this final layer, we pass in the number of classes for the number of neurons. Each neuron represents a class, and the output of this layer will be a 10 neuron vector with each neuron storing some probability that the image in question belongs to the class it represents.\n",
        "\n",
        "Finally, the softmax activation function selects the neuron with the highest probability as its output, voting that the image belongs to that class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdDpZzEQJ4sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(class_num))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7w9B27gUM6D",
        "colab_type": "text"
      },
      "source": [
        "Let's specify the number of epochs we want to train for, as well as the optimizer we want to use.\n",
        "\n",
        "The optimizer is what will tune the weights in your network to approach the point of lowest loss. The Adam algorithm is one of the most commonly used optimizers because it gives great performance on most problems:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyBIcmA3J9n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 25\n",
        "optimizer = 'adam'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3dxvyxBUf4y",
        "colab_type": "text"
      },
      "source": [
        "Let's now compile the model with our chosen parameters. Let's also specify a metric to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1dWpi9jKF6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3vH2duaUifC",
        "colab_type": "text"
      },
      "source": [
        "We can print out the model summary to see what the whole model looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6EfNH2uKNZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5aad1385-1f73-42a9-e3e5-7015f59d1fe2"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,273,706\n",
            "Trainable params: 2,272,362\n",
            "Non-trainable params: 1,344\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABuEc8PcU20e",
        "colab_type": "text"
      },
      "source": [
        "Now we get to training the model. To do this, all we have to do is call the fit() function on the model and pass in the chosen parameters.\n",
        "\n",
        "Here's where I use the seed I chose, for the purposes of reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyNRgg_kKb0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "3385577d-6635-437f-e91b-fa8cd790e0d7"
      },
      "source": [
        "numpy.random.seed(seed)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 452s 578ms/step - loss: 1.5375 - accuracy: 0.4521 - val_loss: 1.2790 - val_accuracy: 0.5468\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 447s 572ms/step - loss: 1.0339 - accuracy: 0.6304 - val_loss: 0.8942 - val_accuracy: 0.6808\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 447s 571ms/step - loss: 0.8512 - accuracy: 0.7019 - val_loss: 0.8077 - val_accuracy: 0.7151\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 441s 565ms/step - loss: 0.7516 - accuracy: 0.7355 - val_loss: 0.6864 - val_accuracy: 0.7582\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 442s 565ms/step - loss: 0.6968 - accuracy: 0.7561 - val_loss: 0.7022 - val_accuracy: 0.7533\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 442s 565ms/step - loss: 0.6528 - accuracy: 0.7707 - val_loss: 0.6890 - val_accuracy: 0.7563\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 443s 567ms/step - loss: 0.6107 - accuracy: 0.7861 - val_loss: 0.6384 - val_accuracy: 0.7823\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 444s 567ms/step - loss: 0.5879 - accuracy: 0.7934 - val_loss: 0.6971 - val_accuracy: 0.7586\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 444s 568ms/step - loss: 0.5705 - accuracy: 0.8026 - val_loss: 0.5512 - val_accuracy: 0.8126\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 443s 566ms/step - loss: 0.5420 - accuracy: 0.8102 - val_loss: 0.5348 - val_accuracy: 0.8129\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 445s 569ms/step - loss: 0.5224 - accuracy: 0.8162 - val_loss: 0.5756 - val_accuracy: 0.8011\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 446s 570ms/step - loss: 0.5087 - accuracy: 0.8222 - val_loss: 0.5383 - val_accuracy: 0.8183\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 443s 567ms/step - loss: 0.4925 - accuracy: 0.8280 - val_loss: 0.5681 - val_accuracy: 0.8043\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 443s 566ms/step - loss: 0.4768 - accuracy: 0.8327 - val_loss: 0.5154 - val_accuracy: 0.8216\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 443s 566ms/step - loss: 0.4694 - accuracy: 0.8344 - val_loss: 0.5054 - val_accuracy: 0.8272\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 443s 566ms/step - loss: 0.4639 - accuracy: 0.8370 - val_loss: 0.5573 - val_accuracy: 0.8072\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 445s 569ms/step - loss: 0.4424 - accuracy: 0.8460 - val_loss: 0.5592 - val_accuracy: 0.8132\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 445s 569ms/step - loss: 0.4436 - accuracy: 0.8449 - val_loss: 0.5293 - val_accuracy: 0.8207\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 445s 569ms/step - loss: 0.4257 - accuracy: 0.8502 - val_loss: 0.5011 - val_accuracy: 0.8299\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 447s 571ms/step - loss: 0.4301 - accuracy: 0.8473 - val_loss: 0.5177 - val_accuracy: 0.8214\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 446s 571ms/step - loss: 0.4253 - accuracy: 0.8509 - val_loss: 0.5772 - val_accuracy: 0.8082\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 444s 567ms/step - loss: 0.4179 - accuracy: 0.8538 - val_loss: 0.5087 - val_accuracy: 0.8265\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 447s 571ms/step - loss: 0.4063 - accuracy: 0.8579 - val_loss: 0.5074 - val_accuracy: 0.8303\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 446s 570ms/step - loss: 0.4003 - accuracy: 0.8598 - val_loss: 0.4911 - val_accuracy: 0.8334\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 443s 567ms/step - loss: 0.3927 - accuracy: 0.8635 - val_loss: 0.5189 - val_accuracy: 0.8251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3d1ac0d198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CklFCmPsKtcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d202a082-29d6-440b-889d-3e43982c4cb4"
      },
      "source": [
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 82.51%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}